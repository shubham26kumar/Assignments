{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671dd451-4eff-479d-9965-fb9f5dc7be26",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6843a04-a480-44c4-a42d-cc891adc94fe",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to transform numerical features in a dataset to a specific range, typically between 0 and 1. This is achieved by subtracting the minimum value of the feature and then dividing by the range (difference between the maximum and minimum values). The purpose of Min-Max scaling is to ensure that all features have the same scale, which can be important for machine learning algorithms that rely on the magnitude of features.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "X normalized = X - Xmin / Xmax - Xmin\n",
    "\n",
    "Where:\n",
    "\n",
    "-->X is the original value of the feature.\n",
    "\n",
    "-->Xmin is the minimum value of the feature in the dataset.\n",
    "\n",
    "-->Xmax is the maximum value of the feature in the dataset.\n",
    "\n",
    "-->Xnormalized is the normalized/scaled value of the feature.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset of ages with values ranging from 20 to 60. You want to apply Min-Max scaling to this dataset. Here's how you would do it:\n",
    "\n",
    "-->Find the minimum and maximum values of the age feature :\n",
    "   \n",
    "   Xmin = 20\n",
    "   \n",
    "   Xmax = 60\n",
    "   \n",
    "-->Apply the Min-Max scaling formula to each age value in the dataset:\n",
    "\n",
    "   For an age value of 30:\n",
    "   \n",
    "   Xnormalized = 30 - 20/60 - 20 = 10/40 = 0.25\n",
    "   \n",
    "   For an age value of 50:\n",
    "   \n",
    "   Xnormalized = 50 - 20/60 - 20 = 30/40 = 0.75\n",
    "\n",
    "By applying Min-Max scaling, you've transformed the original age values into a normalized range between 0 and 1, making them suitable for use in machine learning algorithms. It's important to note that while Min-Max scaling is useful for algorithms that are sensitive to the scale of features (e.g., gradient descent-based algorithms), it might not be necessary or optimal for all types of algorithms. Additionally, outliers in the data can sometimes have a significant impact on the scaling, so it's important to consider the distribution of your data when applying preprocessing techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6b6df-9154-4c26-8732-b1fd2d430175",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277eefed-fa1c-47dd-8896-29ea1035e7a5",
   "metadata": {},
   "source": [
    "Ans :- \n",
    "The Unit Vector technique, also known as vector normalization or unit normalization, is a feature scaling method that transforms the feature vectors in a dataset so that they have a length of 1 (i.e., they become unit vectors). This technique is particularly useful when the direction of the data points is more important than their magnitude. It's commonly used in scenarios where you want to measure the similarity or distance between data points without being influenced by their original magnitudes.\n",
    "\n",
    "The formula for unit vector normalization is as follows:\n",
    "\n",
    "Unit Vector = X / ||X||\n",
    "\n",
    "Where:\n",
    "\n",
    "-->X is the original feature vector.\n",
    "\n",
    "-->||X|| is the Euclidian norm(magnitude) of the feature vector.\n",
    "\n",
    "This technique scales each feature vector by dividing it by its magnitude, resulting in a unit vector that points in the same direction as the original vector but has a length of 1.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "The key difference between Unit Vector normalization and Min-Max scaling is that Min-Max scaling scales the features to a specific range (typically between 0 and 1), whereas Unit Vector normalization only adjusts the magnitude of the vectors, keeping their direction intact.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a dataset with two-dimensional data points representing coordinates on a Cartesian plane. We want to apply Unit Vector normalization to these data points.\n",
    "\n",
    "Original data points:\n",
    "\n",
    "-->Data point A: (3, 4)\n",
    "-->Data point B: (1, 2)\n",
    "\n",
    "1.Calculate the Euclidean norm (magnitude) for each data point:\n",
    "\n",
    "For data point A:\n",
    "\n",
    "||A|| = √(3**2 + 4**2) = 5\n",
    "\n",
    "For data point B:\n",
    "\n",
    "||B|| = √(1**2 + 2**2) = √5 ≈ 2.236\n",
    "\n",
    "2.Apply the Unit Vector normalization formula to each data point:\n",
    "\n",
    "For data point A:\n",
    "\n",
    "Unit Vector(A) = (3,4)/5 = (3/5, 4/5)\n",
    "\n",
    "For data point B:\n",
    "\n",
    "Unit Vector(B) = (1,2)/√5 ≈ (1/2.236, 2/2.236) ≈ (0.447, 0.894)\n",
    "\n",
    "After applying Unit Vector normalization, both data points have been transformed to unit vectors while preserving their directions. The length of each vector is now approximately 1, making them suitable for distance-based calculations such as computing cosine similarity or using them as inputs for machine learning algorithms that rely on direction rather than magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c4ab7-c1ab-43c4-90cb-be76f41575cb",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aef128-0e5a-4345-94f6-3f3b0bd1bd18",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in data analysis and machine learning. It's used to transform high-dimensional data into a new coordinate system, where the new dimensions (called principal components) are orthogonal and sorted in decreasing order of variance. PCA aims to capture the most important patterns or variations in the data by projecting it onto a lower-dimensional space while minimizing information loss.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1.Standardize the Data: Before applying PCA, it's important to standardize the data by subtracting the mean of each feature and dividing by its standard deviation. This step ensures that features with larger scales don't dominate the PCA process.\n",
    "\n",
    "2.Compute Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix shows how different features vary together.\n",
    "\n",
    "3.Compute Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions (principal components) of maximum variance, and the eigenvalues represent the amount of variance captured along each eigenvector.\n",
    "\n",
    "4.Sort Eigenvalues: Sort the eigenvalues in decreasing order. The eigenvectors corresponding to the largest eigenvalues are the most important principal components.\n",
    "\n",
    "5.Select Principal Components: Choose the top k eigenvectors based on how much variance you want to retain in the reduced data. k is the desired number of dimensions in the reduced space.\n",
    "\n",
    "6.Project Data: Project the original data onto the selected k principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset of two features: \"Age\" and \"Income.\" You want to apply PCA to reduce the dimensionality to one dimension for visualization purposes.\n",
    "\n",
    "Original data:\n",
    "\n",
    "Data point 1: Age = 30, Income = 50000\n",
    "Data point 2: Age = 25, Income = 60000\n",
    "Data point 3: Age = 35, Income = 55000\n",
    "\n",
    "1.Standardize the Data: Subtract the mean and divide by the standard deviation for both Age and Income.\n",
    "\n",
    "2.Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "3.Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Sort Eigenvalues: Assume the eigenvalues are sorted as follows:\n",
    "\n",
    "First eigenvalue: 0.08\n",
    "Second eigenvalue: 0.02\n",
    "\n",
    "5.Select Principal Component: Choose the first eigenvector since it corresponds to the largest eigenvalue.\n",
    "\n",
    "6.Project Data: Project the original data onto the first principal component.\n",
    "\n",
    "Projected data:\n",
    "\n",
    "Data point 1: Projected value = 0.6\n",
    "Data point 2: Projected value = 0.8\n",
    "Data point 3: Projected value = 0.7\n",
    "\n",
    "In this example, PCA has reduced the dimensionality from two dimensions (Age and Income) to one dimension (the first principal component), capturing the most important patterns in the data. The projected values can be plotted on a one-dimensional axis for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda4bcc-2aaf-40f0-ad27-4f4f7215bc07",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3d2bd-a3f3-408b-a666-713d39aa8e84",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "PCA and feature extraction are closely related concepts in the context of dimensionality reduction. Feature extraction is a broader term that encompasses various techniques for transforming or representing original features in a way that captures the most important information while reducing dimensionality. PCA is one specific method for feature extraction.\n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features into a new set of features (principal components) that are linear combinations of the original features. These principal components are selected in such a way that they capture the maximum variance present in the data. By focusing on the components with the most variance, PCA helps in retaining the most significant information while reducing the number of dimensions.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with four features: \"Height,\" \"Weight,\" \"Age,\" and \"Income.\" You want to use PCA for feature extraction to reduce the dimensionality of the data while retaining as much variance as possible.\n",
    "\n",
    "Original data (sample):\n",
    "\n",
    "Data point 1: Height = 170 cm, Weight = 65 kg, Age = 30 years, Income = $50,000\n",
    "Data point 2: Height = 160 cm, Weight = 55 kg, Age = 25 years, Income = $60,000\n",
    "Data point 3: Height = 180 cm, Weight = 75 kg, Age = 35 years, Income = $55,000\n",
    "\n",
    "1.Standardize the Data: As in the previous explanation, standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2.Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "3.Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Sort Eigenvalues: Assume the eigenvalues are sorted as follows:\n",
    "\n",
    "First eigenvalue: 0.08\n",
    "Second eigenvalue: 0.05\n",
    "Third eigenvalue: 0.02\n",
    "Fourth eigenvalue: 0.01\n",
    "\n",
    "5.Select Principal Components: Choose the first two eigenvectors (principal components) since they correspond to the largest eigenvalues and capture the most variance.\n",
    "\n",
    "6.Project Data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Projected data:\n",
    "\n",
    "Data point 1: Projected values = (0.6, 0.4)\n",
    "Data point 2: Projected values = (-0.2, -0.1)\n",
    "Data point 3: Projected values = (0.8, -0.3)\n",
    "\n",
    "In this example, PCA has transformed the original four-dimensional data into a two-dimensional representation using the first two principal components. These new features, obtained through PCA, can be used for downstream analysis, visualization, or machine learning tasks. The projected values capture the most significant patterns in the data while reducing the dimensionality, making the data more manageable and potentially improving the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533343c8-b847-41ac-8628-8679bfec0065",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191186a-62f6-487a-9ab4-fddb6b7f7b7a",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "To preprocess the features for building a recommendation system for a food delivery service, you can use Min-Max scaling to ensure that all the features are on a similar scale, which can improve the performance of recommendation algorithms. Here's how you would use Min-Max scaling for each feature:\n",
    "\n",
    "1.Price Feature:\n",
    "Let's say the price of items in your dataset ranges from $5 to $50. Apply Min-Max scaling to transform the price feature to a range between 0 and 1:\n",
    "\n",
    "Scaled Price = Price - Min Price / Max Price - Min Price\n",
    "\n",
    "Where:\n",
    "\n",
    "Price\n",
    "Price is the original price of the item.\n",
    "Min Price\n",
    "Min Price is the minimum price in the dataset (e.g., $5).\n",
    "Max Price\n",
    "Max Price is the maximum price in the dataset (e.g., $50).\n",
    "\n",
    "2.Rating Feature:\n",
    "If the rating of items in your dataset ranges from 1 to 5, you can apply Min-Max scaling to transform the rating feature to a range between 0 and 1:\n",
    "\n",
    "Scaled Rating = Rating - Min Rating / Max Rating - Min Rating\n",
    "\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "Rating\n",
    "Rating is the original rating of the item.\n",
    "Min Rating\n",
    "Min Rating is the minimum rating in the dataset (e.g., 1).\n",
    "Max Rating\n",
    "Max Rating is the maximum rating in the dataset (e.g., 5).\n",
    "\n",
    "3.Delivery Time Feature:\n",
    "If the delivery time of items in your dataset ranges from 20 to 60 minutes, you can apply Min-Max scaling to transform the delivery time feature to a range between 0 and 1:\n",
    "\n",
    "Scaled Delivery Time = Delivery Time - Min Delivery Time / Max Delivery Time - Min Delivery Time\n",
    "\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "Delivery Time\n",
    "Delivery Time is the original delivery time of the item.\n",
    "Min Delivery Time\n",
    "Min Delivery Time is the minimum delivery time in the dataset (e.g., 20 minutes).\n",
    "Max Delivery Time\n",
    "Max Delivery Time is the maximum delivery time in the dataset (e.g., 60 minutes).\n",
    "\n",
    "After applying Min-Max scaling to all the features (price, rating, and delivery time), your data will be transformed so that each feature is within the range of 0 to 1. This ensures that features with different scales don't disproportionately affect the recommendation algorithm. Once the data is scaled, you can proceed with building your recommendation system using techniques like collaborative filtering, content-based filtering, or hybrid approaches, depending on your specific project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b423a-c34c-486f-91e9-3a60660c42e3",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafc7c4-de9b-4142-ae04-046cf86aafb0",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "Using PCA to reduce the dimensionality of a dataset for predicting stock prices can help improve model efficiency, reduce noise, and prevent overfitting. Here's how you can apply PCA to the dataset with many features:\n",
    "\n",
    "1.Data Preparation:\n",
    "\n",
    "Organize your dataset with various features such as company financial data and market trends. Make sure the data is properly cleaned, normalized, and standardized.\n",
    "\n",
    "2.Standardize the Data:\n",
    "\n",
    "Standardize the features by subtracting the mean and dividing by the standard deviation. This step ensures that features with larger scales do not dominate the PCA process.\n",
    "\n",
    "3.Compute Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix of the standardized data. The covariance matrix shows how different features vary together.\n",
    "\n",
    "4.Compute Eigenvectors and Eigenvalues:\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues represent the amount of variance captured along each eigenvector.\n",
    "\n",
    "5.Sort Eigenvalues:\n",
    "\n",
    "Sort the eigenvalues in decreasing order. This allows you to identify which principal components capture the most variance.\n",
    "\n",
    "6.Select Principal Components:\n",
    "\n",
    "Choose the top k eigenvectors based on how much variance you want to retain in the reduced data. You can use methods like explained variance ratio to determine the number of components to retain.\n",
    "\n",
    "7.Project Data onto Principal Components:\n",
    "\n",
    "Project the original data onto the selected k principal components to obtain the lower-dimensional representation. This is done by taking the dot product of the standardized data with the selected eigenvectors.\n",
    "\n",
    "8.Model Building and Evaluation:\n",
    "\n",
    "Use the reduced-dimensional data obtained from PCA as input to build your stock price prediction model. Common algorithms like linear regression, support vector machines, or neural networks can be used. Make sure to split your dataset into training and testing subsets for proper evaluation.\n",
    "\n",
    "It's important to note that while PCA can help reduce dimensionality and enhance model performance, it also results in losing interpretability as the new features are combinations of the original ones. Additionally, when using PCA for stock price prediction, it's crucial to remember that stock prices are influenced by a wide range of factors, including economic indicators, geopolitical events, and market sentiment, which may not be fully captured by financial data and market trends. Therefore, while PCA can be a useful tool, stock price prediction remains a challenging task that requires a comprehensive understanding of financial markets and careful consideration of the data and modeling approaches used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89691362-799d-44cb-af8a-146704656391",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c609a56f-358b-4a64-85de-f02f634274db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "l1 = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(l1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0efd66-0a62-4389-9c63-569f8a3d5db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4b878c-7d96-4049-bdcd-1ba599f3221c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e7ede-6ee0-4c09-b221-69e6fa587718",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddef964-d6a5-4264-9be2-396eebfea90e",
   "metadata": {},
   "source": [
    "Ans :-\n",
    "The decision of how many principal components to retain in PCA depends on the amount of variance you want to capture and the specific goals of your analysis. Typically, you aim to retain enough principal components to explain a high percentage of the total variance in the data while reducing dimensionality.\n",
    "\n",
    "Here's a general process for deciding the number of principal components to retain:\n",
    "\n",
    "1.Compute Explained Variance Ratio:\n",
    "\n",
    "Calculate the explained variance ratio for each principal component. The explained variance ratio for a component \n",
    "i is the proportion of the total variance that is explained by that component.\n",
    "\n",
    "2.Cumulative Explained Variance:\n",
    "\n",
    "Compute the cumulative explained variance by summing up the explained variance ratios from the first component to the \n",
    "ith component. This will give you an idea of how much variance is explained by including \n",
    "i components.\n",
    "\n",
    "3.Decide on Retention Threshold:\n",
    "\n",
    "Set a retention threshold for the cumulative explained variance. This threshold indicates the proportion of total variance you want to retain. A common threshold is often around 90% or higher.\n",
    "\n",
    "4.Choose Number of Components:\n",
    "\n",
    "Determine the number of principal components to retain based on the retention threshold you set. It's usually the number of components needed to exceed or get close to the threshold.\n",
    "\n",
    "5.Visualization and Interpretation:\n",
    "\n",
    "Visualize the cumulative explained variance graph and observe the point where it reaches your chosen threshold. Additionally, consider the interpretability of the principal components – you might choose to retain fewer components if they are easier to interpret.\n",
    "\n",
    "Given that your dataset contains features like height, weight, age, gender, and blood pressure, you can perform PCA to see how much variance is explained by each principal component. You can then decide on the number of components to retain based on your goals and the explained variance.\n",
    "\n",
    "Note that the \"gender\" feature is categorical, so you would need to encode it using techniques like one-hot encoding before performing PCA. The \"blood pressure\" feature might require careful handling as well, depending on its format (e.g., systolic and diastolic readings).\n",
    "\n",
    "In practice, you might start by retaining a higher number of components and then analyze the cumulative explained variance to determine how many components are sufficient for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3948925-feac-4309-9c04-e5a04ca5d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [\n",
    "    [165, 60, 30, 'Male', 120],\n",
    "    [175, 70, 25, 'Female', 110],\n",
    "    [180, 80, 35, 'Male', 130],\n",
    "    [160, 50, 28, 'Female', 115],\n",
    "    # ... more data points ...\n",
    "]\n",
    "\n",
    "#Separate categorical and numerical features\n",
    "categorical_features = [3] #Index of the gender feature\n",
    "numerical_features = [0, 1, 2, 4] #Index of height, weight, age and blood pressure features\n",
    "\n",
    "# Extract numerical data\n",
    "numerical_data = [row[numerical_features[0]] for row in data]\n",
    "data1 = np.array(numerical_data)\n",
    "data2 = np.reshape(data1,[-1,1])\n",
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data2)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Choose number of principal components\n",
    "# You can use explained_variance_ratio_ to understand how much variance is explained by each component\n",
    "# and decide how many components to retain based on your goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483bdab-c3f5-422f-be49-42d4f9802d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
